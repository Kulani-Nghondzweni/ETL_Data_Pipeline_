# ğŸ› ï¸ ETL Data Pipeline 

A simple ETL (Extract, Transform, Load) data pipeline built with Python. This project demonstrates core data engineering concepts by processing data from a CSV file, transforming it using `pandas`, and loading it into a SQLite database.

---

## ğŸš€ Overview

This project was created as part of a Data Engineering self testing project to demonstrate:

- ğŸ“¥ **Extraction** of structured data from a CSV file
- ğŸ”„ **Transformation** of raw data (e.g., cleaning, computed columns)
- ğŸ—ƒï¸ **Loading** the transformed data into a SQL database (SQLite)

It serves as a minimal working example of building a modular and testable ETL pipeline.

---

## ğŸ“‚ Project Structure

ETL-DATA-PIPELINE/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ (csv files once downloaded) 
â”‚
â”œâ”€â”€ (etl_database.db once saved)
â”‚
â”œâ”€â”€ etl/
â”‚ â””â”€â”€ main.py 
â”‚
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_etl.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

## Setup

Before running the ETL pipeline, make sure to download the necessary CSV files.  
These files are not included in the repository due to size limitations and are required for the ETL process to work correctly.

Place the CSV files in the `data/` directory as expected by the pipeline:


Once the files are in place, you can run the ETL process as usual:


python etl/main.py



### Download CSV Files

You can download the CSV files from:

curl  https://storage.googleapis.com/bdt-beam/users_v.csv -o data/users.csv

curl  https://storage.googleapis.com/bdt-beam/orders_v_2022.csv -o data/orders.csv
